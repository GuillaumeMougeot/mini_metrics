import argparse
import numpy as np
import pandas as pd
from sklearn.metrics import confusion_matrix

#-------------------------------------------------------------------------------

# Compute prediction level
def compute_prediction_level(df):
    """This function choose the lowest level for which the confidence of the model is above the level threshold.
    """

    results = []
    for instance_id, group in df.groupby("instance_id"):
        # Sort levels numerically or lexicographically
        group = group.sort_values("level")

        # Filter levels where confidence >= threshold
        valid_levels = group[group["confidence"] >= group["threshold"]]

        if not valid_levels.empty:
            chosen_level = valid_levels.iloc[0]["level"]  # lowest valid level
        else:
            chosen_level = None  # or np.nan

        results.append({"instance_id": instance_id, "prediction_level": chosen_level})

    # Convert results to DataFrame and merge back
    result_df = pd.DataFrame(results)
    df = df.merge(result_df, on="instance_id", how="left")

    return df

#-------------------------------------------------------------------------------

# Accuracy
def micro_accuracy(df):
    return df['correct'].mean()

# Macro accuracy at each level
def accuracy_score(y_true, y_pred, balanced=True, adjusted=False):
    """Implementation based on https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html
    """
    C = confusion_matrix(y_true, y_pred)
    if balanced:
        with np.errstate(divide="ignore", invalid="ignore"):
            per_class = np.diag(C) / C.sum(axis=1)
        if np.any(np.isnan(per_class)):
            # warnings.warn("y_pred contains classes not in y_true")
            per_class = per_class[~np.isnan(per_class)]
        score = np.mean(per_class)
        if adjusted:
            n_classes = len(per_class)
            if n_classes > 1: # bug fix if only one y_true class has been detected
                chance = 1 / n_classes
                score -= chance
                score /= 1 - chance
    else:
        score = np.diag(C).sum() / C.sum()
    return float(score)

# Coverage
def coverage(df):
    """Proportion of instances where the model made any prediction
      (i.e., had confidence â‰¥ threshold at some level).
    """
    return df['prediction_made'].mean()

# Coverage per level
def coverage_per_level(df):
    return df['prediction_level'].value_counts(dropna=False).sort_index()/len(df)

# Correct @ Level
def correct_at_each_level(df):
    level_accuracy = df.dropna().groupby('prediction_level')['correct'].mean()
    return level_accuracy.to_dict()

# Average Prediction Level
def average_prediction_level(df):
    return df['prediction_level'].mean()

# No Prediction Rate
def no_prediction_rate(df):
    return 1 - coverage(df)

# Mean Confidence of Correct vs Incorrect Predictions
def confidence_stats(df):
    result = {}
    for outcome in [0, 1]:
        ids = df.loc[df['correct'] == outcome, 'instance_id']
        subset = df[df['instance_id'].isin(ids)]
        result[f'mean_confidence_correct_{outcome}'] = subset['confidence'].mean()
    return result

def hierarchical_metric(df, rewards=None, penalties=None):
    """Global hierarchical metric.
    """
    if rewards is None:
        # rewards = lambda level: (3-level)/6
        rewards = pd.Series([1/2 - x/6 for x in range(3)]) # [1/2, 1/3, 1/6]
    if penalties is None:
        # penalties = lambda level: (-1-level)/6)
        penalties = pd.Series([-(1+x)/6 for x in range(3)]) # [-1/6, -1/3, -1/2]
    m = np.where(
    df['confidence']>df['threshold'], 
    np.where(
        df['label']==df['prediction'],
        rewards[df['level']], 
        penalties[df['level']]),
    0)
    return sum(m)/df['instance_id'].nunique()

# Run all metrics in one call
def evaluate_all_metrics(df):
    return {
        'micro_accuracy': micro_accuracy(df),
        'coverage': coverage(df),
        'coverage_per_level' : coverage_per_level(df),
        'average_prediction_level': average_prediction_level(df),
        'correct_at_each_level': correct_at_each_level(df),
        **confidence_stats(df),
        "hierarchical_metric":hierarchical_metric(df),
    }

def main(csv="mini_results.csv"):
    df = pd.read_csv(csv)
    df=compute_prediction_level(df)
    print(evaluate_all_metrics(df))

def cli():
    parser = argparse.ArgumentParser()
    parser.add_argument("-f", "--file", default="mini_results.csv", help="Path to the result files.")
    args = parser.parse_args()
    main(args.file)

if __name__=='__main__':
    cli()